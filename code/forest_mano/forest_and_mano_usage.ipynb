{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial for using Forest to analyze Beiwe data. We will first download the data using mano. We will also be creating some time series plots using the generated statistic summaries. There are four parts to this tutorial.\n",
    "\n",
    "1. Check Python version and download Forest.\n",
    "2. Download data for your study from the server.\n",
    "3. Explore the file structure of your data\n",
    "4. Process data using forest.\n",
    "5. Creating time series plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Python Version and Download Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we need to check the current distribution of Python. Note that forest is built using Python 3.11. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the python version and the path to the Python interpreter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.5\n",
      "/n/onnela_dp_l3/Lab/envs/.forest_venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "print(python_version()) ## Prints your version of python\n",
    "print(sys.executable) ## Prints your current python installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The output should display two lines.* \n",
    "\n",
    "1. The Python version installed- make sure you are not using a version of Python that is earlier than 3.11\n",
    "2. The path to where Python is currently installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may need to install git, pip, mano and forest. To do so, either run the chunk below (the one with lines starting with \"!\") or enter the lines below (not starting with \"!\" in a command-line shell. If you already have mano and forest installed, you can skip to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mano in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (0.5.2)\n",
      "Requirement already satisfied: requests in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from mano) (2.32.3)\n",
      "Requirement already satisfied: lxml in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from mano) (5.3.1)\n",
      "Requirement already satisfied: python-dateutil in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from mano) (2.9.0.post0)\n",
      "Requirement already satisfied: cryptease in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from mano) (0.2.0)\n",
      "Requirement already satisfied: cryptography in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from cryptease->mano) (44.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from python-dateutil->mano) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from requests->mano) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from requests->mano) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from requests->mano) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from requests->mano) (2025.1.31)\n",
      "Requirement already satisfied: cffi>=1.12 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from cryptography->cryptease->mano) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from cffi>=1.12->cryptography->cryptease->mano) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting https://github.com/onnela-lab/forest/tarball/develop\n",
      "  Using cached https://github.com/onnela-lab/forest/tarball/develop\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: holidays in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (0.68)\n",
      "Requirement already satisfied: librosa in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (0.10.2.post1)\n",
      "Requirement already satisfied: numpy in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (2.1.3)\n",
      "Requirement already satisfied: openrouteservice in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (2.3.3)\n",
      "Requirement already satisfied: pandas in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (2.2.3)\n",
      "Requirement already satisfied: pyproj in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (3.7.1)\n",
      "Requirement already satisfied: python-dateutil in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (2025.1)\n",
      "Requirement already satisfied: ratelimit in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (2.2.1)\n",
      "Requirement already satisfied: requests in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (2.32.3)\n",
      "Requirement already satisfied: scipy in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (1.15.2)\n",
      "Requirement already satisfied: shapely in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (2.0.7)\n",
      "Requirement already satisfied: ssqueezepy in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (0.6.5)\n",
      "Requirement already satisfied: timezonefinder in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from beiwe-forest==1.0rc2) (6.5.8)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from librosa->beiwe-forest==1.0rc2) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from librosa->beiwe-forest==1.0rc2) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from librosa->beiwe-forest==1.0rc2) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from librosa->beiwe-forest==1.0rc2) (5.2.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from librosa->beiwe-forest==1.0rc2) (0.61.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from librosa->beiwe-forest==1.0rc2) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from librosa->beiwe-forest==1.0rc2) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from librosa->beiwe-forest==1.0rc2) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from librosa->beiwe-forest==1.0rc2) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from librosa->beiwe-forest==1.0rc2) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from librosa->beiwe-forest==1.0rc2) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from requests->beiwe-forest==1.0rc2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from requests->beiwe-forest==1.0rc2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from requests->beiwe-forest==1.0rc2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from requests->beiwe-forest==1.0rc2) (2025.1.31)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from pandas->beiwe-forest==1.0rc2) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from python-dateutil->beiwe-forest==1.0rc2) (1.17.0)\n",
      "Requirement already satisfied: cffi<2,>=1.15.1 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from timezonefinder->beiwe-forest==1.0rc2) (1.17.1)\n",
      "Requirement already satisfied: h3>4 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from timezonefinder->beiwe-forest==1.0rc2) (4.2.1)\n",
      "Requirement already satisfied: pycparser in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from cffi<2,>=1.15.1->timezonefinder->beiwe-forest==1.0rc2) (2.22)\n",
      "Requirement already satisfied: packaging in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from lazy-loader>=0.1->librosa->beiwe-forest==1.0rc2) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from numba>=0.51.0->librosa->beiwe-forest==1.0rc2) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from pooch>=1.1->librosa->beiwe-forest==1.0rc2) (4.3.6)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (from scikit-learn>=0.20.0->librosa->beiwe-forest==1.0rc2) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: orjson in /n/onnela_dp_l3/Lab/envs/.forest_venv/lib/python3.12/site-packages (3.10.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#run this chunk to install mano and forest\n",
    "%pip install mano \n",
    "%pip install --upgrade https://github.com/onnela-lab/forest/tarball/develop\n",
    "%pip install orjson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# Or, copy and paste the below lines into a command-line shell` \n",
    "\n",
    "`pip install mano`\n",
    "\n",
    "`pip install https://github.com/onnela-lab/forest/tarball/develop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In this notebook, you will install the develop branch of forest. This branch has all of the most recent features (including location type information), but function names are slightly different than in the main branch, so they may not match what is on the website. To find documentation specific to the develop branch, look at the current version's docstring by typing a function name and holding shift+tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Beiwe Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will download data from a beiwe study. Edit the cell below to match parameters in your study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For **study_id**, enter the \"study ID, found in the top right corner of the study page\". \n",
    "- For **direc**, the current working directory will be used. If you want data to be stored in another directory, change this variable to another string with the desired filepath. \n",
    "- For **dest_folder_name**, enter the \"name of the folder you want raw data stored in\". \n",
    "- For **server**, enter the server where data is located. If your Beiwe website URL starts with studies.beiwe.org, enter \"studies\"\n",
    "- For **time_start**, enter the earliest date you want to download data for, in YYYY-MM-DD format.\n",
    "- For **time_end**, enter the latest date you want to download data for, in YYYY-MM-DD format. If this is None, mano will download all data available (up until today at midnight). \n",
    "- For **data_streams**, enter a list of data streams you want to download. Forest currently analyzes `gps`, `survey_timings`, `calls`, and `texts` data streams. A full list of data types can be found under the \"Download Data\" tab of the Beiwe website. If this is None, all possible data streams will be downloaded. \n",
    "- For **beiwe_ids**, enter a list of Beiwe IDs you want to download data for. If you leave this as an empty list, mano will attempt to download data for all user IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_id: 1\n",
      "/n/onnela_dp_l3/Lab/HOPE/beiwe/data/trial_phase2_download_participants.csv\n",
      "Created On                                    2018-05-22\n",
      "Patient ID                                      2n18iikg\n",
      "Status                                          Inactive\n",
      "OS Type                                              IOS\n",
      "First Registration Date        2018-06-06 19:00:00 (UTC)\n",
      "Last Registration                                    NaN\n",
      "Last Upload                                          NaN\n",
      "Last Survey Download                                 NaN\n",
      "Last Set Password                                    NaN\n",
      "Last Push Token Update                               NaN\n",
      "Last Device Settings Update                          NaN\n",
      "Last OS Version                                      NaN\n",
      "App Version Code                                     NaN\n",
      "App Version Name                                     NaN\n",
      "Last Heartbeat                                       NaN\n",
      "Name: 1, dtype: object\n",
      "2018-06-06\n",
      "beiwe_ids: ['2n18iikg']\n",
      "destination directory: /n/onnela_dp_l3/Lab/HOPE/beiwe/data/bulk_download_test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd \n",
    "\n",
    "# Test for one HOPE study, one user\n",
    "# in the order of: DFCI_Wright_HOPE Trial Phase 2, DFCI_Wright_HOPE Trial Phase 2_Passive Data Only, DFCI_Wright_HOPE Trial, DFCI_Wright_HOPE Troubleshooting, DFCI_Wright_HOPE Test, DFCI_Wright_HOPE_Test 2\n",
    "HOPE_STUDY_IDS = ['598365d5388cd66a62ac1f9e', '5a2ae1dc03d3c425ef0ea752', '588224eff4d48a76f488cdfd',\n",
    "                  '5a79f17d03d3c45080924ed4', '59c2b5b4388cd6715a958247', '5h7D9XT2vrN3BWkdcbYVNtpI']\n",
    "study_id = HOPE_STUDY_IDS[0]  # test on one part of the study\n",
    "direc = os.path.abspath(\"../../data/\")  # set to the desired directory\n",
    "# to be used to pull participant ids\n",
    "\n",
    "# TODO change destination folder name to reflect the data being downloaded\n",
    "# TODO change to reflect the data being downloaded\n",
    "\n",
    "dest_folder_name = \"bulk_download_test\"\n",
    "server = \"studies\"\n",
    "# time_start = \"2017-01-01\"\n",
    "# time_end = \"2017-01-02\"\n",
    "\n",
    "\n",
    "# For Jupyter Notebook ONLY\n",
    "sys.argv = ['script_name', '1']  \n",
    "task_id = int(sys.argv[1])\n",
    "print(\"task_id: \" + str(task_id))\n",
    "\n",
    "METADATA_TB_PATH = os.path.join(direc, \"trial_phase2_download_participants.csv\")\n",
    "print(METADATA_TB_PATH)\n",
    "\n",
    "#==================             ========================\n",
    "\n",
    "user_params = pd.read_csv(METADATA_TB_PATH) # this is the file that contains the user parameters\n",
    "row = user_params.iloc[task_id]\n",
    "print(row)\n",
    "\n",
    "# study_id = row['study id'].values[0]\n",
    "# direc = os.getcwd() #current working directory, \n",
    "# dest_folder_name = \"raw_data\"\n",
    "\n",
    "server = \"studies\"\n",
    "\n",
    "time_start = row['First Registration Date'].split(\" \")[0]\n",
    "print(time_start)\n",
    "time_end = None\n",
    "\n",
    "# data_streams = [\"gps\", \"survey_timings\", \"survey_answers\", \"audio_recordings\", \"calls\", \"texts\", \"accelerometer\"]\n",
    "# test gps for now -- change to include all passive features\n",
    "data_streams = [\"gps\", \"accelerometer\", \"survey_timings\", \"survey_answers\"] #, \"survey_timings\", \"survey_answers\", \"audio_recordings\", \"calls\", \"texts\", \"accelerometer\"]\n",
    "\n",
    "beiwe_ids = [row['Patient ID']]\n",
    "print(\"beiwe_ids: \" + str(beiwe_ids))\n",
    "\n",
    "# if dest_dir doesn't exist, create it\n",
    "dest_dir = os.path.join(direc, dest_folder_name)\n",
    "print(\"destination directory: \" + dest_dir)\n",
    "\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next cell, we will import our keyring_studies.py file which includes download credentials. If you haven't already done this, open the keyring_studies.py file and paste your credentials inside. \n",
    "\n",
    "If your keyring_studies.py file is in a different directory than the one which includes this notebook, replace `sys.path.insert(0, '')` with `sys.path.insert(0, 'path/to/dir/containing/file/')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import .py file located in another directory if needed\n",
    "import mano\n",
    "import sys\n",
    "sys.path.insert(0, direc)\n",
    "\n",
    "import keyring_studies\n",
    "kr = mano.keyring(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(beiwe_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell will download your data. Downloading your data will probably be the most time-consuming part of the whole process, so if you've already downloaded the data, you will save time by not running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for 2n18iikg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from helper_functions import download_data\n",
    "download_data(kr, study_id, dest_dir, beiwe_ids, time_start, time_end, data_streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/n/home01/egraff/.conda/envs/ood_env/lib/python3.12/site-packages/ipykernel_launcher.py', '-f', '/n/home01/egraff/.local/share/jupyter/runtime/kernel-3b51dcad-d842-4f70-9c8f-db222c47b6eb.json']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(sys.argv)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m job_id = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# job_id = 0\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mjob_id = \u001b[39m\u001b[33m'\u001b[39m + \u001b[38;5;28mstr\u001b[39m(job_id))\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# # -------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# # manually edited params\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# # -------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m \n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m#     print('FINISHED user_id = ' + user_id)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: '-f'"
     ]
    }
   ],
   "source": [
    "# unit testing bulk data processing code \n",
    "\n",
    "# NOTE: in publicly available version of this file, some variable values \n",
    "# have been replaced by a placeholder / \"foo\" value. \n",
    "\n",
    "from platform import python_version\n",
    "from datetime import date, datetime\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import mano\n",
    "import mano.sync as msync\n",
    "import numpy as np\n",
    "\n",
    "print(sys.argv)\n",
    "job_id = int(sys.argv[1]) # job_id = 0\n",
    "print('job_id = ' + str(job_id))\n",
    "\n",
    "# # -------------------------------------------------------------------------------\n",
    "# # manually edited params\n",
    "# # -------------------------------------------------------------------------------\n",
    "\n",
    "# beiwe_id_list_path    = \"foo/beiwe_2_0_start_and_end_dates_clean.csv\"\n",
    "# beiwe_data_dir        = 'foo'\n",
    "# beiwe_data_error_dir  = 'foo'\n",
    "# keyring_dir           = 'foo'\n",
    "\n",
    "# # study id\n",
    "# study_id = 'foo'\n",
    "\n",
    "# # set up the keyring\n",
    "# sys.path.insert(0, keyring_dir)\n",
    "# import keyring_studies_MK\n",
    "# Keyring = mano.keyring(None)\n",
    "\n",
    "\n",
    "# # -------------------------------------------------------------------------------\n",
    "# # run\n",
    "# # -------------------------------------------------------------------------------\n",
    "\n",
    "# # read data frame with study users\n",
    "# beiwe_id_df = pd.read_csv(beiwe_id_list_path, sep =',')\n",
    "# beiwe_id_df = beiwe_id_df[beiwe_id_df['beiwe_id'].notna()]\n",
    "# # beiwe_id_df.columns.values\n",
    "# beiwe_id_df_rows = list(range(beiwe_id_df.shape[0]))\n",
    "# execute_indices = [i for i in beiwe_id_df_rows if i % 15 == job_id]\n",
    "\n",
    "# # iterate over subset of indexes of beiwe_id_df\n",
    "# for row_i in execute_indices: # row_i = 0\n",
    "\n",
    "#     # -------------------------------------------------------------------------------\n",
    "#     # pull data specific to particular user\n",
    "#     # -------------------------------------------------------------------------------\n",
    "#     # beiwe_id_df.index[beiwe_id_df['beiwe_id'] == '114akcoc'].tolist()\n",
    "#     user_id = beiwe_id_df['beiwe_id'].tolist()[row_i]\n",
    "#     # bug fixed: replaced [job_id] with [row_i]\n",
    "#     day_date_min = beiwe_id_df['beiwe_data_query_start'].tolist()[row_i]\n",
    "#     day_date_max = beiwe_id_df['beiwe_data_query_end'].tolist()[row_i]\n",
    "#     print('Starting user_id = ' + user_id)\n",
    "\n",
    "#     # create sequence of days over which we will be quering data day by day\n",
    "#     download_rn_start = datetime.strptime(day_date_min, '%Y-%m-%d').date()\n",
    "#     # download_rn_end   = date.today()\n",
    "#     download_rn_end   = datetime.strptime(day_date_max, '%Y-%m-%d').date()\n",
    "#     download_days_seq = pd.date_range(download_rn_start, download_rn_end, freq = 'd').tolist()\n",
    "#     download_days_seq = [str(i.date()) + \"T00:00:00\" for i in download_days_seq]\n",
    "\n",
    "#     # -------------------------------------------------------------------------------\n",
    "#     # run download day by day: gps\n",
    "#     # -------------------------------------------------------------------------------\n",
    "#     j_range_max = len(download_days_seq) - 1\n",
    "#     for j in range(j_range_max): # j = 2\n",
    "#         # define currently considered\n",
    "#         time_start = download_days_seq[j]\n",
    "#         time_end = download_days_seq[j + 1]\n",
    "#         # wheter keep repeating\n",
    "#         repeat_cnt = 0\n",
    "#         while True:\n",
    "#             # download GPS\n",
    "#             try:\n",
    "#                 zf = msync.download(Keyring, study_id, user_id, data_streams = ['gps'], time_start = time_start, time_end = time_end)\n",
    "#                 zf.extractall(beiwe_data_dir)\n",
    "#                 # if successful, break\n",
    "#                 break\n",
    "#             except BaseException as e:\n",
    "#                 print(str(e))\n",
    "#                 repeat_cnt = repeat_cnt + 1\n",
    "#                 print('repeat_cnt = repeat_cnt + 1 [gps] -- ' + user_id + \" \" + str(j))\n",
    "#             # if exceeded the number of breaks\n",
    "#             if repeat_cnt > 10:\n",
    "#                  # append error log\n",
    "#                 if not os.path.exists(beiwe_data_error_dir):\n",
    "#                     os.makedirs(beiwe_data_error_dir)\n",
    "#                 file_tmp = os.path.join(beiwe_data_error_dir, user_id + \"_gps_\" + str(j))\n",
    "#                 open(file_tmp, mode = 'a').close()\n",
    "#                 break\n",
    "\n",
    "#     print('FINISHED user_id = ' + user_id)\n",
    "\n",
    "print('--- FINISHED PYTHON SCRIPT RUN --- job_id = ' + str(job_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1s5wlcm6', '2n18iikg', '2umdx87r', '41n4f9xk', '4aczi869', '4kp3bpt3', '4uu7odcm', '4yqhd5tz', '56l6mrgc', '6pqstf1g', '7ijvywnb', '81rasxas', '8a4lsxex', '8lqhw9ep', '93ubkqfi', '9r91qo8x', 'aej6wxh8', 'aj6hu6x', 'akka6vtq', 'azybbrqe', 'bhjfpzgc', 'bntzix8l', 'boai4zds', 'btjka62m', 'c3rv1bs9', 'ckud9s4q', 'cpqdi58s', 'erv2mapd', 'fc2k5drf', 'g5pvhkig', 'gb4hxmdq', 'h1qpaxs7', 'h4yepx8s', 'hfnit4ev', 'hrqqx8bu', 'i5b2fdez', 'i9fqxgwn', 'igfq43wg', 'j47zkf1o', 'j8soi1xe', 'jg55qc21', 'jxfij7hr', 'k2rf8qm4', 'kkxkaagr', 'kzt9osem', 'mihep4', 'ox6k2lbi', 'peeff4t8', 'q2zyj5m4', 'r7dszv23', 'rccun', 'rckq58a9', 's18ydzme', 'srpx1ilr', 'tg9vyfot', 'u2l3u6og', 'ua1djdlg', 'uiwlw4n5', 'ujg255xj', 'ujjuisc1', 'utff7e4t', 'uwk9bwdt', 'uz1v2g7u', 'vyck9k79', 'x8wr4182', 'xcdydxji', 'xfd4twop', 'ychlwvnz', 'zj4gbhlp', 'zo8vuchq']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Erase once the script download_data.py is tested and working\n",
    "\n",
    "# script to download certain passive data covariates from the HOPE dataset\n",
    "# references\n",
    "# forest_and_manu_usage.ipynb\n",
    "# https://github.com/onnela-lab/als-beiwe-passive-data/blob/main/py/download_data_gps_AWS.py \n",
    "# TODO refactor to download all passive data covariates for all users in the HOPE study\n",
    "\n",
    "# pasted from forest_and_mano_usage.ipynb; generalized to download for all participants in a given study\n",
    "# from platform import python_version\n",
    "# import sys\n",
    "\n",
    "# print(python_version()) ## Prints your version of python\n",
    "# print(sys.executable) ## Prints your current python installation\n",
    "\n",
    "# #run this chunk to install mano and forest\n",
    "# %pip install mano \n",
    "# %pip install --upgrade https://github.com/onnela-lab/forest/tarball/develop\n",
    "# %pip install orjson\n",
    "\n",
    "import os\n",
    "# Test for one HOPE study, one user\n",
    "# in the order of: DFCI_Wright_HOPE Trial Phase 2, DFCI_Wright_HOPE Trial Phase 2_Passive Data Only, DFCI_Wright_HOPE Trial, DFCI_Wright_HOPE Troubleshooting, DFCI_Wright_HOPE Test, DFCI_Wright_HOPE_Test 2\n",
    "HOPE_STUDY_IDS = ['598365d5388cd66a62ac1f9e', '5a2ae1dc03d3c425ef0ea752', '588224eff4d48a76f488cdfd', '5a79f17d03d3c45080924ed4', '59c2b5b4388cd6715a958247', '5h7D9XT2vrN3BWkdcbYVNtpI'] \n",
    "study_id = HOPE_STUDY_IDS[0] # test on one part of the study \n",
    "direc = os.path.abspath(\"../../data/\") # set to the desired directory\n",
    "# to be used to pull participant ids\n",
    "PART_TB_PATH = direc + \"/download_participants/trial_phase2_download_participants.csv\"\n",
    "dest_folder_name = \"bulk_download_test\"\n",
    "server = \"studies\"\n",
    "time_start = \"2017-01-01\"\n",
    "time_end = \"2017-01-02\" # TODO for now test one day's to make sure data download script runs end to end\n",
    "# for now focus on passive data - specifically gps and accelerometer\n",
    "data_streams = [\"gps\", \"accelerometer\", \"survey_timings\", \"survey_answers\"] #, \"survey_timings\", \"survey_answers\", \"audio_recordings\", \"calls\", \"texts\", \"accelerometer\"]\n",
    "# for now test on one user\n",
    "# then pull all user ids from participant table \n",
    "import pandas as pd\n",
    "\n",
    "# TODO: rename participant table with study id, rather than hardcoding \n",
    "beiwe_id_df = pd.read_csv(PART_TB_PATH, sep = \",\")\n",
    "\n",
    "# convert patient id to string and then to list\n",
    "beiwe_id_df['Patient ID'] = beiwe_id_df['Patient ID'].astype(str)\n",
    "beiwe_ids = beiwe_id_df['Patient ID'].tolist()\n",
    "print(beiwe_ids) # sanity check \n",
    "\n",
    "# beiwe_ids = ['u2l3u6og'] \n",
    "\n",
    "dest_dir = os.path.join(direc, dest_folder_name)\n",
    "\n",
    "# import .py file located in another directory if needed\n",
    "import mano\n",
    "import sys\n",
    "sys.path.insert(0, direc)\n",
    "\n",
    "import keyring_studies\n",
    "kr = mano.keyring(None)\n",
    "\n",
    "\n",
    "# from helper_functions import download_data\n",
    "# download_data(kr, study_id, dest_dir, beiwe_ids, time_start, time_end, data_streams) \n",
    "\n",
    "# up until cell 5 in the forest_mano notebook ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1s5wlcm6', '2n18iikg', '2umdx87r', '41n4f9xk', '4aczi869', '4kp3bpt3', '4uu7odcm', '4yqhd5tz', '56l6mrgc', '6pqstf1g', '7ijvywnb', '81rasxas', '8a4lsxex', '8lqhw9ep', '93ubkqfi', '9r91qo8x', 'aej6wxh8', 'aj6hu6x', 'akka6vtq', 'azybbrqe', 'bhjfpzgc', 'bntzix8l', 'boai4zds', 'btjka62m', 'c3rv1bs9', 'ckud9s4q', 'cpqdi58s', 'erv2mapd', 'fc2k5drf', 'g5pvhkig', 'gb4hxmdq', 'h1qpaxs7', 'h4yepx8s', 'hfnit4ev', 'hrqqx8bu', 'i5b2fdez', 'i9fqxgwn', 'igfq43wg', 'j47zkf1o', 'j8soi1xe', 'jg55qc21', 'jxfij7hr', 'k2rf8qm4', 'kkxkaagr', 'kzt9osem', 'mihep4', 'ox6k2lbi', 'peeff4t8', 'q2zyj5m4', 'r7dszv23', 'rccun', 'rckq58a9', 's18ydzme', 'srpx1ilr', 'tg9vyfot', 'u2l3u6og', 'ua1djdlg', 'uiwlw4n5', 'ujg255xj', 'ujjuisc1', 'utff7e4t', 'uwk9bwdt', 'uz1v2g7u', 'vyck9k79', 'x8wr4182', 'xcdydxji', 'xfd4twop', 'ychlwvnz', 'zj4gbhlp', 'zo8vuchq']\n"
     ]
    }
   ],
   "source": [
    "# convert patient id to string and then to list\n",
    "beiwe_id_df['Patient ID'] = beiwe_id_df['Patient ID'].astype(str)\n",
    "beiwe_ids = beiwe_id_df['Patient ID'].tolist()\n",
    "print(beiwe_ids)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(beiwe_ids)\n",
    "beiwe_ids_list = beiwe_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can directly explore the structure of the sample Beiwe data that we've just downloaded. \n",
    "\n",
    "At the top level of the directory `/data`, subject-level data is separately contained with subdirectories. Each subdirectory are named according to the subject's assigned Beiwe ID. In this sample, we observe the six subdirectories each from a separate study participant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulk_download_test\n",
      "├── 2n18iikg\n",
      "├── 4aczi869\n",
      "├── 4kp3bpt3\n",
      "├── 2umdx87r\n",
      "├── 41n4f9xk\n",
      "├── 56l6mrgc\n",
      "├── 4uu7odcm\n",
      "├── 7ijvywnb\n",
      "├── 93ubkqfi\n",
      "├── azybbrqe\n",
      "├── aej6wxh8\n",
      "├── akka6vtq\n",
      "├── 8lqhw9ep\n",
      "├── bntzix8l\n",
      "├── aj6hu6x\n",
      "├── bhjfpzgc\n",
      "├── boai4zds\n",
      "├── 9r91qo8x\n",
      "├── erv2mapd\n",
      "├── gb4hxmdq\n",
      "├── fc2k5drf\n",
      "├── btjka62m\n",
      "└── h1qpaxs7\n",
      "\n",
      "23 directories\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import tree\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "tree(dest_dir, level=1, limit_to_directories=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data using Forest \n",
    "- Using the Forest library developed by the Onnela lab, we compute daily GPS and communication summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate the GPS-related summary statistics by using the **gps_stats_main** function under the **traj2stat.py** in the Jasmine tree of Forest. This code will take between 15 minutes to 12 hours to run, depending on your machine and the quantity of data downloaded. To make sure that everything is working right, change the `beiwe_ids` argument from `None` to a list with just a couple of the Beiwe IDs in your study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For **data_dir**, enter the \"path to the data file directory\". This will be the same directory you downloaded data into.\n",
    "- For **output_dir**, enter the \"path to the file directory where output is to be stored\". \n",
    "- For **tz_str**, enter the time zone where the study was conducted. Here, it's **\"America/New_York.\"** We can use \"pytz.all_timezones\" to check all options.\n",
    "- For **frequency**, there are 'daily' or 'hourly' or 'both' for the temporal resolution for summary statistics. Currently, one must pass this as one of the Frequency class imported from Jasmine. So, you may use Frequency.HOURLY or Frequency.DAILY\n",
    "- For **save_traj**, it's \"True\" if you want to save the trajectories as a csv file, \"False\" if you don't (default: False). Here, we chose **\"True.\"**\n",
    "- For **beiwe_ids**, enter the list of Beiwe IDs to run Forest on. If this is `None`, jasmine will run on all users in the data_dir directory.\n",
    "- For **places_of_interest**, enter a list of places of interest. This list must contain keywords from [openstreetmaps](https://wiki.openstreetmap.org/wiki/OpenStreetBrowser/Category_list)\n",
    "\n",
    "There are also more optional arguments that can be passed to the function, which are located in the Hyperparameters class in the traj2stat.py file. These include:\n",
    "- For **log_threshold**, enter the number of minutes required to be spent at a place to count as a place\n",
    "- For **save_osm_log**, enter whether you want to save the log associated with places of interest.\n",
    "\n",
    "and others as can been seen in the class definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from forest.jasmine.traj2stats import gps_stats_main, Hyperparameters\n",
    "from forest.constants import Frequency\n",
    "\n",
    "data_dir = dest_dir\n",
    "gps_output_dir = \"gps_output\"\n",
    "tz_str = \"America/New_York\"\n",
    "freq = Frequency.DAILY\n",
    "save_traj = True \n",
    "beiwe_ids = [\"2n18iikg\"] # Test for one person\n",
    "places_of_interest = None\n",
    "\n",
    "# if you are not interested in more specific hyperparameters, you can use the default ones\n",
    "# by setting parameters = None or not passing in the parameters argument\n",
    "parameters = Hyperparameters()\n",
    "parameters.save_osm_log = False\n",
    "parameters.log_threshold = 60 # threshold, in minutes, for logging locations if OSM analysis is enabled\n",
    "parameters.pcr_bool = True # enables physical circadian rhythm (PCR) statistics\n",
    "parameters.pcr_window = 14 # number of days to look back and forward for calculating PCR\n",
    "parameters.pcr_sample_rate = 30 # sample rate in seconds\n",
    "\n",
    "gps_stats_main(\n",
    "    data_dir, gps_output_dir, tz_str, freq, save_traj, places_of_interest = places_of_interest, \n",
    "    participant_ids = beiwe_ids, parameters = parameters\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The output should describe how the data is being processed. If this is working correctly, you will see something like:*\n",
    "    \n",
    "><i>User: tcqrulfj  \n",
    "Read in the csv files ...  \n",
    "Collapse data within 10 second intervals ...  \n",
    "Extract flights and pauses ...  \n",
    "Infer unclassified windows ...  \n",
    "Merge consecutive pauses and bridge gaps ...  \n",
    "Selecting basis vectors ...  \n",
    "Imputing missing trajectories ...  \n",
    "Tidying up the trajectories...  \n",
    "Calculating the daily summary stats...<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liz testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jy8yzsap', 'md7fnll7', 'wrb5oh7u', 'wgs5rptp', 'q21jny47', '52td8unr', '67zqexic', 'piiua4us', '4458ann3', '7h42nxij', 's18ydzme', 'em2heqoc', 'boai4zds', 'qrb9cncj', '1s5wlcm6', 'aj6hu6x', 'nzvk7jwo', 'behdfa31', 'bhjfpzgc', 'igfq43wg', 'j3ba35d2', 'h599npeu', 'xcdydxji', '2n18iikg', 'cpjgpdpc', 'sgjviesj', '4kp3bpt3', '4zqq46uj', 'srpx1ilr', 'd3o6kuvf', 'nnu1qurv', 'btjka62m', '7ijvywnb', 'ua1djdlg', 'dglk2mak', 'ud1ic49g', 'unlkfxnc', 'sg5x9we2', 'i5b2fdez', '3j2fdbb', 'jg55qc21', '4s7o5pps', '9r91qo8x', 'kzt9osem', 'asj59ze4', 'akka6vtq', 'o912nw3y', 'aej6wxh8', 'vby6fos4', 'fxh8kqh4', 'j8soi1xe', 'o3ze4d97', '8lpaia8q', 'uiwlw4n5', 'ujg255xj', 'lglv5j7s', 'hfnit4ev', 'see4r8y6', '8lqhw9ep', 't9xq7k8b', '5iv5o89o', '83i73qyl', '4fyd1ssv', 'k93dkv8w', 'h4yepx8s', 'fc2k5drf', 'rmtdpger', 'mk9ee8p4', '56l6mrgc', '1rkt87d2', '8r4hkqi6', '4aczi869', 'ychlwvnz', 'ukexs41t', '76rm9dnl', 'ige4zl6o', 'x8wr4182', 'qkcrsaa9', 'i9fqxgwn', 'kkxkaagr', 'pwby6ex2', '6ogn9wsa', '3tg2dbdl', 'n7g4xhkg', '93ubkqfi']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dest_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m beiwe_ids = beiwe_id_df[\u001b[33m\"\u001b[39m\u001b[33mbeiwe_id\u001b[39m\u001b[33m\"\u001b[39m].tolist()\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(beiwe_ids)  \n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m tree(\u001b[43mdest_dir\u001b[49m, level=\u001b[32m1\u001b[39m, limit_to_directories=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# from forest.jasmine.traj2stats import gps_stats_main, Hyperparameters\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# from forest.constants import Frequency\u001b[39;00m\n\u001b[32m     41\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     69\u001b[39m \n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# print(f\"GPS data processing complete. Output saved to {os.path.join(direc, 'gps_summaries.csv')}\")\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'dest_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Next, we can directly explore the structure of the sample Beiwe data that we've just downloaded. \n",
    "\n",
    "# At the top level of the directory `/data`, subject-level data is separately contained with subdirectories. Each subdirectory are named according to the subject's assigned Beiwe ID. In this sample, we observe the six subdirectories each from a separate study participant. \n",
    "import os\n",
    "from helper_functions import tree\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from forest.jasmine.traj2stats import gps_stats_main, Hyperparameters\n",
    "from forest.constants import Frequency\n",
    "import sys\n",
    "import glob\n",
    "import yaml\n",
    "\n",
    "# Get the absolute path to the directory this script lives in\n",
    "SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "# Construct the path to the config file relative to the script\n",
    "CONFIG_PATH = os.path.join(SCRIPT_DIR, \"../../config/HOPE_config.yaml\")\n",
    "# # quick and dirty hardcoding for testing\n",
    "# CONFIG_PATH = \"/n/onnela_dp_l3/Lab/HOPE/beiwe/config/HOPE_config.yaml\"\n",
    "CONFIG_DIR = os.path.dirname(CONFIG_PATH)\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# data_dir = os.path.abspath(os.path.join(CONFIG_DIR, config[\"data_dir\"]))\n",
    "raw_data_dir = os.path.abspath(os.path.join(CONFIG_DIR, config[\"raw_data_dir\"]))\n",
    "raw_data_p\n",
    "metadata_path = os.path.abspath(os.path.join(CONFIG_DIR, config[\"metadata_path\"]))\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(raw_data_dir, ))\n",
    "\n",
    "beiwe_id_df = pd.read_csv(metadata_path, sep = \",\")\n",
    "\n",
    "# convert patient id to string and then to list\n",
    "beiwe_id_df[\"beiwe_id\"] = beiwe_id_df[\"beiwe_id\"].astype(str)\n",
    "beiwe_ids = beiwe_id_df[\"beiwe_id\"].tolist()\n",
    "print(beiwe_ids)  \n",
    "\n",
    "tree(dest_dir, level=1, limit_to_directories=True)\n",
    "\n",
    "# from forest.jasmine.traj2stats import gps_stats_main, Hyperparameters\n",
    "# from forest.constants import Frequency\n",
    "\n",
    "# user_params = pd.read_csv(metadata_path) # this is the file that contains the user parameters\n",
    "# row = user_params.iloc[task_id]\n",
    "# print(row)\n",
    "# beiwe_ids = [row[\"beiwe_id\"]]\n",
    "# print(\"beiwe ids: \" + str(beiwe_ids), flush=True)\n",
    "\n",
    "gps_output_dir = \"gps_output\"\n",
    "tz_str = \"America/New_York\"\n",
    "freq = Frequency.HOURLY\n",
    "save_traj = True \n",
    "# TEST ON SINGLE SUBJECT\n",
    "beiwe_ids = [\"d3o6kuvf\"] # Test one id for now \"d3o6kuvf\" smallish dataset\n",
    "\n",
    "places_of_interest = None\n",
    "# if you are not interested in more specific hyperparameters, you can use the default ones\n",
    "# by setting parameters = None or not passing in the parameters argument\n",
    "parameters = Hyperparameters()\n",
    "parameters.save_osm_log = False\n",
    "parameters.log_threshold = 60 # threshold, in minutes, for logging locations if OSM analysis is enabled\n",
    "parameters.pcr_bool = True # enables physical circadian rhythm (PCR) statistics\n",
    "parameters.pcr_window = 14 # number of days to look back and forward for calculating PCR\n",
    "parameters.pcr_sample_rate = 30 # sample rate in seconds\n",
    "\n",
    "gps_stats_main(\n",
    "    data_dir, gps_output_dir, tz_str, freq, save_traj, places_of_interest = places_of_interest, \n",
    "    participant_ids = beiwe_ids, parameters = parameters\n",
    ")\n",
    "\n",
    "from helper_functions import concatenate_summaries\n",
    "\n",
    "\n",
    "concatenate_summaries(dir_path = os.path.join(direc, gps_output_dir), \n",
    "                      output_filename = os.path.join(direc,\"gps_summaries.csv\"))\n",
    "\n",
    "print(f\"GPS data processing complete. Output saved to {os.path.join(direc, 'gps_summaries.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/n/onnela_dp_l3/Lab/HOPE/beiwe/data/raw/manual_HOPE_paper2_download'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(raw_data_dir, \"manual_HOPE_paper2_download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now contatenate GPS summaries into one file. \n",
    "\n",
    ">*Note- this function appends the frequency value to the end of the filename e.g. \"gps_summaries_daily.csv\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import concatenate_summaries\n",
    "\n",
    "\n",
    "concatenate_summaries(dir_path = os.path.join(direc, gps_output_dir), \n",
    "                      output_filename = os.path.join(direc,\"gps_summaries.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we compute the call and text-based summary statistics by using the **log_stats_main** function under the **log_stats.py** in the Willow tree of Forest. This should run a lot faster than `forest.jasmine.traj2stats.gps_stats_main`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For **data_dir**, enter the \"path to the data file directory\". \n",
    "- For **output_dir**, enter the \"path to the file directory where output is to be stored\". \n",
    "- For **tz_str**, enter the time zone where the study was conducted. Here, it's **\"America/New_York.\"** \n",
    "- For **option**, choose a Frequency value corresponding to the temporal resolution you would like data to be aggregated to. \n",
    "- For **beiwe_ids**, enter the list of Beiwe IDs to run Forest on. If this is `None`, willow will run on all users in the data_dir directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import forest.willow.log_stats\n",
    "data_dir = dest_dir\n",
    "comm_output_dir = \"comm_output\"\n",
    "tz_str = \"America/New_York\"\n",
    "option = Frequency.DAILY\n",
    "beiwe_ids = None\n",
    "\n",
    "\n",
    "\n",
    "forest.willow.log_stats.log_stats_main(\n",
    "    data_dir, comm_output_dir, tz_str, option, beiwe_ids = beiwe_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The output should describe how the data is being processed (e.g., read, collapse, extracted...imputing, tidying, and calculating daily summary stats).*\n",
    "\n",
    ">*Note- calls and texts data are only collected on Android phones. If you only enrolled users with iPhones in your study, you will not have any output here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following code is  used to concatenate these files into a single csv for the **communication summaries**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import concatenate_summaries\n",
    "\n",
    "concatenate_summaries(dir_path = os.path.join(direc,comm_output_dir), \n",
    "                      output_filename = os.path.join(direc,\"comm_summaries.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The output should show the data for the first five observations in the concatenated dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we summarize survey information using the **survey_stats_main** function under the **base.py** in the Sycamore tree of Forest. This will take between 5 minutes and 2 hours to run, depending on how many surveys were administered durinng your study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For **data_dir**, enter the \"path to the data file directory\". \n",
    "- For **output_dir**, enter the \"path to the file directory where output is to be stored\". \n",
    "- For **tz_str**, enter the time zone where the study was conducted. Here, it's **\"America/New_York.\"** \n",
    "- For **beiwe_ids**, enter the list of Beiwe IDs to run Forest on. If this is `None`, sycamore will run on all users in the data_dir directory.\n",
    "- For **config_path**, enter the filepath to your downloaded survey config file. This can be downloaded by clicking \"edit study\" on your study page, and clicking \"Export study settings JSON file under \"Export/Import study settings\". If this is None, Sycamore will still run, but fewer outputs will be produced. \n",
    "- For **interventions_filepath**, enter the filepath to your downloaded interventions timing file. This can be downloaded by clicking \"edit study\" on your study page, and clicking \"Download Interventions\" next to \"Intervention Data\". If this is None, Sycamore will still run, but fewer outputs will be produced. (note, this doesn't apply if you are using the main version of sycamore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forest.sycamore.base import compute_survey_stats\n",
    "\n",
    "data_dir = dest_dir\n",
    "survey_output_dir = \"survey_output\"\n",
    "tz_str = \"America/New_York\"\n",
    "beiwe_ids = None\n",
    "config_path = None\n",
    "interventions_filepath = None\n",
    "\n",
    "compute_survey_stats(\n",
    "    study_folder = data_dir, output_folder = survey_output_dir,\n",
    "    config_path = config_path, tz_str = tz_str, users = beiwe_ids,\n",
    "    start_date = time_start, end_date = time_end, \n",
    "    interventions_filepath = interventions_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we summarize accelerometer using the **run** function under the **base.py** in the Oak tree of Forest. This tree is in beta testing, so don't be surprised if you encounter errors running this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For **data_dir**, enter the \"path to the data file directory\". \n",
    "- For **accelerometer_output_dir**, enter the \"path to the file directory where output is to be stored\". \n",
    "- For **tz_str**, enter the time zone where the study was conducted. Here, it's **\"America/New_York.\"** \n",
    "- For **frequency**, choose a value of frequency similar as what was used in jasmine. \n",
    "- For **beiwe_ids**, enter the list of Beiwe IDs to run Forest on. If this is `None`, willow will run on all users in the data_dir directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forest.oak.base import run\n",
    "\n",
    "data_dir = dest_dir\n",
    "accelerometer_output_dir = \"accel_output\"\n",
    "tz_str = \"America/New_York\"\n",
    "frequency = Frequency.DAILY\n",
    "beiwe_ids = None\n",
    "\n",
    "run(data_dir, accelerometer_output_dir, \n",
    "    tz_str, frequency, users = beiwe_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import concatenate_summaries\n",
    "\n",
    "\n",
    "concatenate_summaries(dir_path = os.path.join(direc, accelerometer_output_dir), \n",
    "                      output_filename = os.path.join(direc,\"accel_summaries.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will also be generate some time series plots using the generated statistic summaries.\n",
    "- To read the file, we need to define **response_filename** with the concatenated dataset. Here, we are using 'gps_summaries_daily.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "direc = os.getcwd()\n",
    "response_filename = 'gps_summaries_daily.csv'\n",
    "path_resp = os.path.join(direc, response_filename)    \n",
    "\n",
    "# read data\n",
    "response_data = pd.read_csv(path_resp)\n",
    "\n",
    "# GPS data (jasmine)\n",
    "response_data['Date'] = pd.to_datetime(response_data[['year', 'month', 'day']])\n",
    "\n",
    "# Accelerometer data (oak)\n",
    "# response_data.rename(columns={'date': 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to be sorted according to date. The following code will sort and create 4 even time intervals in the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure the data is sorted according to date\n",
    "response_data.sort_values('Date', inplace = True)\n",
    "response_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "def time_series_plot(var_to_plot, ylab = '', xlab = 'Date', num_x_ticks = 4):\n",
    "    for key, grp in response_data.groupby(['Beiwe_ID']):\n",
    "        plt.plot(response_data.Date, response_data[var_to_plot], label=key)\n",
    "    \n",
    "    #if len(response_data['Beiwe_ID'].unique()) > 1: ## more than one user to plot\n",
    "    #    plt.plot(response_data.Date, response_data[var_to_plot], c=response_data['Beiwe_ID'].astype('category'))\n",
    "    #else:\n",
    "    #    plt.plot(response_data.Date, response_data[var_to_plot]) #just one user\n",
    "    title = f\"Time Series Plot of {var_to_plot}\"\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    \n",
    "    ## get evenly indices\n",
    "    tick_indices = [(i * (len(response_data.Date.unique()) - 1)) // (num_x_ticks - 1) for i in range(num_x_ticks) ]\n",
    "    \n",
    "    plt.xticks(response_data.Date.unique()[tick_indices])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can now create time series plots using **time_series_plot('variable')**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_plot('dist_traveled', ylab = \"km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The output displays a time series plot for the variable, \"dist_traveled.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_plot('sd_flight_length', ylab = \"km\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The output displays a time series plot for the variable, \"sd_flight_length.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forest_venv",
   "language": "python",
   "name": "forest_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
